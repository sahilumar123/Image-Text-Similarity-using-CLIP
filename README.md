# Image-Text-Similarity-using-CLIP
Description: This project demonstrates how to measure semantic similarity between images and text using the CLIP (Contrastive Language–Image Pretraining) model. Implemented in Google Colab, the system encodes both images and textual descriptions into a shared embedding space and calculates similarity to identify the best matching image–text pairs.
